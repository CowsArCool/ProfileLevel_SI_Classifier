{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Default Packages\n",
                "import os\n",
                "import sys\n",
                "import pickle\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import os.path as path\n",
                "\n",
                "# Torch\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "# Using standard huggingface tokenizer for compatability\n",
                "from transformers import (BertTokenizer, BertModel, AdamW, \n",
                "                          get_linear_schedule_with_warmup)\n",
                "\n",
                "# PyTorch Lightning\n",
                "import pytorch_lightning as pl\n",
                "from pytorch_lightning.metrics.functional import accuracy, f1, auroc, recall, precision\n",
                "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
                "from pytorch_lightning.loggers import WandbLogger\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class RedditImplicit (Dataset):\n",
                "    def __init__ (self, reddit_df, tokenizer, max_example_len = 512):\n",
                "        # src is divided into input_ids, token_type_ids, and attention_mask\n",
                "        self.src = tokenizer (\n",
                "            reddit_df['text'].tolist(), \n",
                "            add_special_tokens = True, \n",
                "            truncation = True, \n",
                "            padding = \"max_length\", \n",
                "            return_attention_mask = True, \n",
                "            return_tensors = \"pt\",\n",
                "            max_length = max_example_len\n",
                "        )\n",
                "\n",
                "        self.trg = reddit_df['label'].replace ({'non-suicide':0, 'suicide':1}).tolist()\n",
                "\n",
                "\n",
                "    @staticmethod\n",
                "    def custom_vocab_preprocessing(df):\n",
                "        return df['text']\n",
                "\n",
                "    def __getitem__(self, idx):\n",
                "        return [\n",
                "            tuple([self.src['input_ids'][idx], self.src['attention_mask'][idx]]),\n",
                "            torch.tensor(self.trg[idx])\n",
                "        ]\n",
                "\n",
                "    def __len__(self):\n",
                "        assert len(self.src['input_ids']) == len(self.trg)\n",
                "        return len(self.trg)\n",
                "\n",
                "    def __str__(self):\n",
                "        return f'RedditImplicit ({self.dataset_percent*100}% of full dataset)'\n",
                "        \n",
                "\n",
                "class RedditImplicitDataModule (pl.LightningDataModule):\n",
                "    def __init__(\n",
                "        self, data:pd.DataFrame, \n",
                "        tokenizer, splits:list =[1], \n",
                "        max_example_len: int = 512, \n",
                "        shuffle: bool = True,\n",
                "        batch_size:int = 32, \n",
                "        num_workers:int = 0\n",
                "    ):\n",
                "    \n",
                "        super().__init__()\n",
                "\n",
                "        self.tokenizer = tokenizer\n",
                "        self.batch_size = batch_size\n",
                "        self.max_example_len = max_example_len\n",
                "        self.shuffle = shuffle\n",
                "        self.num_workers = num_workers\n",
                "\n",
                "        self.df_splits = list()\n",
                "        datalen= len(data)\n",
                "        for i, split_percent in enumerate(splits):\n",
                "            prev_split = sum(splits[:i])\n",
                "\n",
                "            self.df_splits.append(\n",
                "                data[\n",
                "                    int(prev_split*datalen):\n",
                "                    int((prev_split *datalen) + \n",
                "                        (split_percent*datalen)\n",
                "                    )\n",
                "                ]\n",
                "            )\n",
                "\n",
                "    def setup (self, stage=None):\n",
                "        self.splits = [\n",
                "            RedditImplicit (\n",
                "                data, \n",
                "                self.tokenizer,\n",
                "                self.max_example_len\n",
                "            )\n",
                "            for data in self.df_splits\n",
                "        ]\n",
                "\n",
                "\n",
                "        if len(self.splits) <= 3:\n",
                "            # complicated syntax making it possible to assign all three at once while padding\n",
                "            # validset/testset if there arent enough splits to fill those values\n",
                "            self.trainset, self.validset, self.testset = [\n",
                "                split for split in self.splits] + [self.splits[-1]]*(3 - len(self.splits))\n",
                "\n",
                "            self.datasets = {\n",
                "                'train': self.trainset,\n",
                "                'valid': self.validset,\n",
                "                'test': self.testset\n",
                "            }\n",
                "\n",
                "\n",
                "    def train_dataloader (self):\n",
                "        return DataLoader(\n",
                "            self.trainset,\n",
                "            batch_size=self.batch_size,\n",
                "            shuffle=self.shuffle,\n",
                "            num_workers=self.num_workers\n",
                "        )\n",
                "\n",
                "\n",
                "    def val_dataloader (self):\n",
                "        return DataLoader(\n",
                "            self.validset,\n",
                "            batch_size=self.batch_size,\n",
                "            shuffle=False,\n",
                "            num_workers=self.num_workers\n",
                "        )\n",
                "\n",
                "\n",
                "    def test_dataloader (self):\n",
                "        return DataLoader(\n",
                "            self.testset,\n",
                "            batch_size=self.batch_size,\n",
                "            shuffle=False,\n",
                "            num_workers=self.num_workers\n",
                "        )\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SuicideClassifier (pl.LightningModule):\n",
                "    def __init__ (\n",
                "        self, output_classes:list = ['suicide'], \n",
                "        training_steps:int = None, \n",
                "        warmup_steps:int = 0, lr=None,\n",
                "        metrics = []):\n",
                "        \n",
                "        super().__init__()\n",
                "        \n",
                "        self.training_steps = training_steps\n",
                "        self.warmup_steps = warmup_steps\n",
                "        self.output_classes = output_classes\n",
                "        self.output_dim = len (output_classes)\n",
                "\n",
                "        self.bert = BertModel.from_pretrained(BERT_MODEL, return_dict=True)\n",
                "        self.ff = nn.Linear (self.bert.config.hidden_size, self.output_dim)\n",
                "        self.output_norm = nn.Sigmoid ()\n",
                "\n",
                "        # loss loss function\n",
                "        self.criterion = nn.BCELoss()\n",
                "        self.lr = lr\n",
                "        \n",
                "        self.metrics = metrics\n",
                "        self.implemented_metrics = {\n",
                "            'ROC':self.calculate_ROC,\n",
                "            'binary_report':self.calculate_binary_report\n",
                "        }\n",
                "        \n",
                "\n",
                "    def forward (self, input_ids, attention_mask, \n",
                "                 labels =None, normalize= True):\n",
                "        \"\"\" Preforms a forward pass through the model \n",
                "            and runs loss calculations\n",
                "\n",
                "        Args:\n",
                "            input_ids (torch.tensor[N, max_example_len]): integer incoded words\n",
                "            attention_mask (torch.tensor[N, max_example_len]): mask for self attention (1:unmasked, 0: masked)\n",
                "            labels (torch.tensor [N]): ground truth y values for batch\n",
                "        \"\"\"\n",
                "        # if attention_mask is None:\n",
                "        #     attention_mask = torch.ones_like(input_ids)\n",
                "        \n",
                "        # with return_dict=True, bert outputs\n",
                "        x = self.bert (input_ids, attention_mask=attention_mask)\n",
                "        y_hat = self.ff (x.pooler_output)\n",
                "        if normalize:\n",
                "            y_hat = self.output_norm (y_hat)\n",
                "        \n",
                "        if self.output_dim==1:\n",
                "            y_hat = torch.squeeze(y_hat)\n",
                "\n",
                "        loss = 0\n",
                "        if labels is not None:\n",
                "            # print (f'y_hat type {(y_hat.dtype)}, labels type {(labels.dtype)}')\n",
                "            loss = self.criterion (y_hat, labels.type (torch.float32))\n",
                "\n",
                "        return loss, y_hat\n",
                "\n",
                "    def _step (self, batch, step_type):\n",
                "        (input_ids, attention_mask), labels = batch\n",
                "        loss, output = self (input_ids, attention_mask, labels)\n",
                "        self.log ('{}_loss'.format(step_type), loss, prog_bar=True, logger=True)\n",
                "\n",
                "        return {f'loss':loss, f'output':output, f'labels': labels}\n",
                "\n",
                "    def training_step (self, batch, batch_idx):\n",
                "        values = self._step (batch, 'train')\n",
                "        return values\n",
                "\n",
                "    def validation_step (self, batch, batch_idx):\n",
                "        values = self._step (batch, 'valid')\n",
                "        return values\n",
                "\n",
                "    def test_step (self, batch, batch_idx):\n",
                "        values = self._step (batch, 'test')\n",
                "        return values['loss']\n",
                "    \n",
                "    def calculate_ROC (self, preds, labels, step_type):\n",
                "        for i, name in enumerate(self.output_classes):\n",
                "            if self.output_dim ==1:\n",
                "                # class_roc_auc = auroc (preds, labels)\n",
                "                i = None\n",
                "\n",
                "            class_roc_auc = auroc(preds[:, i], labels[:, i], pos_label=1)\n",
                "            \n",
                "            self.log (\n",
                "                f\"{name}_roc_auc/{step_type}\", class_roc_auc, self.current_epoch\n",
                "            )\n",
                "            \n",
                "    def calculate_binary_report(self, preds, labels, step_type):\n",
                "        assert len(labels.shape)==1, 'binary report is reserved for output_dim==1'\n",
                "        assert len(preds.shape)==1\n",
                "        \n",
                "        # print (f'shapes| preds: {preds.shape} labels: {labels.shape} types| preds: {preds.dtype} labels: {labels.dtype}')\n",
                "        \n",
                "        binary_metrics = {\n",
                "            'accuracy':[accuracy],\n",
                "            'f1 score':[f1, {'num_classes':1}],\n",
                "            'precision':[precision],\n",
                "            'recall_count':[recall]\n",
                "        }\n",
                "        \n",
                "        for name, metric_info, in binary_metrics.items():\n",
                "            kwargs = {}\n",
                "            if len (metric_info) >1:\n",
                "                kwargs=metric_info[1]\n",
                "            \n",
                "            self.log ('{}/{}'.format (name, step_type), metric_info[0](preds, labels, **kwargs))        \n",
                "\n",
                "    \n",
                "    def log_metrics (self, outputs, step_type):\n",
                "        labels, preds = [], []\n",
                "        \n",
                "        for output in outputs:\n",
                "            for out_labels in output[\"labels\"].detach().cpu():\n",
                "                labels.append(out_labels)\n",
                "            for out_predictions in output[\"output\"].detach().cpu():\n",
                "                preds.append(out_predictions)\n",
                "\n",
                "\n",
                "        labels = torch.stack (labels).int()\n",
                "        preds = torch.stack (preds)\n",
                "\n",
                "        for metric in self.metrics:\n",
                "            self.implemented_metrics[metric](preds, labels, step_type)\n",
                "\n",
                "    def training_epoch_end (self, outputs):\n",
                "        self.log_metrics (outputs, 'train')\n",
                "        \n",
                "        return            \n",
                "    \n",
                "    def validation_epoch_end (self, outputs):\n",
                "        self.log_metrics (outputs, 'valid')\n",
                "        \n",
                "        return \n",
                "            \n",
                "    def configure_optimizers (self):\n",
                "        optimizer = AdamW (self.parameters (), lr = self.lr)\n",
                "\n",
                "        scheduler = get_linear_schedule_with_warmup (\n",
                "            optimizer, \n",
                "            num_warmup_steps = self.warmup_steps,\n",
                "            num_training_steps = self.training_steps\n",
                "        )\n",
                "\n",
                "        return {\n",
                "            'optimizer':optimizer,\n",
                "            'lr_scheduler':{\n",
                "                'scheduler':scheduler,\n",
                "                'interval':'step'\n",
                "            }\n",
                "        }\n",
                "\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "RUNNING_DIR = r'C:\\Code\\NLP\\ProfileLevel_SI_Classifier'\n",
                "datasets_dir = path.join(RUNNING_DIR, 'Datasets')\n",
                "\n",
                "BERT_MODEL = 'bert-base-uncased'\n",
                "CLASSES = ['suicidal']\n",
                "BATCH_SIZE = 12\n",
                "NUM_EPOCHS = 10\n",
                "# LEARNING_RATE = 2e-5\n",
                "LEARNING_RATE = 1.5e-5\n",
                "PATIENCE = 2\n",
                "MAX_EXAMPLE_LEN =100\n",
                "\n",
                "N_EXAMPLES = 3000\n",
                "\n",
                "\n",
                "# Data Loading\n",
                "reddit_df = pd.read_csv (path.join (datasets_dir, 'Implicitly_Labeled_Suicide_Reddit.csv'))[['text', 'class']]\n",
                "reddit_df = reddit_df.sample (N_EXAMPLES).reset_index(drop=True)\n",
                "reddit_df.rename (columns= {'class':'label'}, inplace=True)\n",
                "\n",
                "# Tokenization and Batching\n",
                "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL)\n",
                "data_module = RedditImplicitDataModule (\n",
                "    reddit_df,\n",
                "    tokenizer,\n",
                "    splits=[0.8,0.2],\n",
                "    max_example_len = MAX_EXAMPLE_LEN, \n",
                "    shuffle=True,\n",
                "    batch_size=BATCH_SIZE,\n",
                ")\n",
                "\n",
                "training_steps = (len(reddit_df)//BATCH_SIZE)*NUM_EPOCHS"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = SuicideClassifier (\n",
                "    output_classes= CLASSES,\n",
                "    training_steps = training_steps,\n",
                "    warmup_steps=training_steps/5,\n",
                "    lr=LEARNING_RATE, \n",
                "    metrics=['ROC','binary_report']\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "with open (path.join(RUNNING_DIR, 'words.txt')) as f:\n",
                "  display_name = '-'.join (np.random.choice ((''.join (f.readlines()).split ('\\n')), size=2))\n",
                "  \n",
                "wandb_logger = WandbLogger(\n",
                "  name = display_name,\n",
                "  project=\"BERT Implicitly Labeled Reddit v2\",\n",
                "  config = {\n",
                "    'model':BERT_MODEL,\n",
                "    'classes':CLASSES,\n",
                "    'batch_size':BATCH_SIZE,\n",
                "    'num_epochs':NUM_EPOCHS,\n",
                "    'learning_rate':LEARNING_RATE,\n",
                "    'early_stopping_patience':PATIENCE,\n",
                "    'max_example_len':MAX_EXAMPLE_LEN,\n",
                "    'n_examples':N_EXAMPLES,\n",
                "  }\n",
                ")\n",
                "\n",
                "save_dir = path.join (RUNNING_DIR, 'model_checkpoints', display_name)\n",
                "os.makedirs(save_dir)\n",
                "early_stopping_callback = EarlyStopping(\n",
                "  monitor='valid_loss', patience=PATIENCE)\n",
                "checkpoint_callback = ModelCheckpoint(\n",
                "  dirpath=save_dir,\n",
                "  filename=\"best-checkpoint\",\n",
                "  save_top_k=1,\n",
                "  verbose=True,\n",
                "  monitor=\"valid_loss\",\n",
                "  mode=\"min\"\n",
                ")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "trainer = pl.Trainer(\n",
                "  logger=wandb_logger,\n",
                "  checkpoint_callback=checkpoint_callback,\n",
                "  callbacks=[early_stopping_callback],\n",
                "  max_epochs=NUM_EPOCHS,\n",
                "  gpus=1,\n",
                "  progress_bar_refresh_rate=30\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "trainer.fit(model, data_module)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "trainer.test()\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "loaded_model = SuicideClassifier.load_from_checkpoint(\n",
                "  path.join (RUNNING_DIR, 'model_checkpoints', 'walk-antiques', 'best-checkpoint.ckpt'),\n",
                ")\n",
                "\n",
                "twitter_trainer = pl.Trainer(\n",
                "  logger=wandb_logger,\n",
                "  checkpoint_callback=checkpoint_callback,\n",
                "  callbacks=[early_stopping_callback],\n",
                "  max_epochs=NUM_EPOCHS,\n",
                "  gpus=1,\n",
                "  progress_bar_refresh_rate=30\n",
                ")\n",
                "\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Profile_SI-env",
            "language": "python",
            "name": "profile_si-env"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.2"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
