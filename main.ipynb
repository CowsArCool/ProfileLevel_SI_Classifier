{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default Packages\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os.path as path\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "# Using standard huggingface tokenizer for compatability\n",
    "from transformers import (BertTokenizer, BertModel, \n",
    "                          get_linear_schedule_with_warmup)\n",
    "\n",
    "# PyTorch Lightning\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.metrics.functional import accuracy, f1, auroc, recall, precision\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "# Internal Packages\n",
    "from datasets import *\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_wandb (project, display_name, lr):\n",
    "    return WandbLogger(\n",
    "        name = display_name,\n",
    "        project=project,\n",
    "        config = {\n",
    "            'model':BERT_MODEL,\n",
    "            'classes':CLASSES,\n",
    "            'batch_size':BATCH_SIZE,\n",
    "            'num_epochs':NUM_EPOCHS,\n",
    "            'learning_rate':lr,\n",
    "            'early_stopping_patience':PATIENCE,\n",
    "            'max_example_len':MAX_EXAMPLE_LEN,\n",
    "            'n_examples':N_EXAMPLES,\n",
    "        }\n",
    "    )\n",
    "\n",
    "def generate_callbacks (display_name):\n",
    "    save_dir = path.join (RUNNING_DIR, 'model_checkpoints', display_name)\n",
    "    os.makedirs(save_dir)\n",
    "    early_stopping_callback = EarlyStopping(\n",
    "        monitor='valid_loss', patience=PATIENCE)\n",
    "    \n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=save_dir,\n",
    "        filename=\"best-checkpoint\",\n",
    "        save_top_k=1,\n",
    "        verbose=True,\n",
    "        monitor=\"valid_loss\",\n",
    "        mode=\"min\"\n",
    "    )\n",
    "    \n",
    "    return checkpoint_callback, early_stopping_callback\n",
    "\n",
    "def generate_trainer_params (project, lr):\n",
    "    with open (path.join(RUNNING_DIR, 'words.txt')) as f:\n",
    "        display_name = '-'.join (np.random.choice ((''.join (f.readlines()).split ('\\n')), size=2))\n",
    "    print ('Using display_name: {} for project: {}'.format(display_name, project))\n",
    "        \n",
    "    wandb_logger = prepare_wandb (project, display_name, lr)\n",
    "    callbacks = generate_callbacks (display_name)\n",
    "    \n",
    "    return {\n",
    "        'logger':wandb_logger,\n",
    "        'checkpoint_callback':callbacks[0],\n",
    "        'callbacks':[callbacks[1]]\n",
    "    }\n",
    "\n",
    "def generate_trainer (trainer_params):\n",
    "    return pl.Trainer(\n",
    "        max_epochs=NUM_EPOCHS,\n",
    "        progress_bar_refresh_rate=30,\n",
    "        gpus=1,\n",
    "        **trainer_params\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_twitterSI(data_path):\n",
    "    df = pd.read_csv(data_path)\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def read_redditSI (data_path):\n",
    "    # Data Loading\n",
    "    reddit_df = pd.read_csv (data_path)[['text', 'class']]\n",
    "    reddit_df = reddit_df.sample (N_EXAMPLES).reset_index(drop=True)\n",
    "    reddit_df.rename (columns= {'class':'label'}, inplace=True)\n",
    "    \n",
    "    return reddit_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUNNING_DIR = r'C:\\Code\\NLP\\ProfileLevel_SI_Classifier'\n",
    "datasets_dir = path.join(RUNNING_DIR, 'Datasets')\n",
    "\n",
    "BERT_MODEL = 'bert-base-uncased'\n",
    "CLASSES = ['suicidal']\n",
    "BATCH_SIZE = 12\n",
    "NUM_EPOCHS = 10\n",
    "# LEARNING_RATE = 2e-5\n",
    "R_LEARNING_RATE = 1.5e-5\n",
    "T_LEARNING_RATE = 1.5e-5\n",
    "PATIENCE = 2\n",
    "MAX_EXAMPLE_LEN =100\n",
    "\n",
    "N_EXAMPLES = 3000\n",
    "\n",
    "reddit_df = read_redditSI (\n",
    "    path.join (datasets_dir, 'Implicitly_Labeled_Suicide_Reddit.csv'))\n",
    "twitter_df = read_twitterSI(\n",
    "    path.join (datasets_dir, 'Origional Suicidal Tweets.csv'))\n",
    "\n",
    "# Tokenization and Batching\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_MODEL)\n",
    "reddit_data_module = RedditImplicitDataModule (\n",
    "    reddit_df,\n",
    "    tokenizer,\n",
    "    splits=[0.8,0.2],\n",
    "    max_example_len = MAX_EXAMPLE_LEN, \n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "twitter_data_module = TwitterDataModule (\n",
    "    twitter_df,\n",
    "    tokenizer,\n",
    "    splits=[0.7,0.3],\n",
    "    max_example_len = MAX_EXAMPLE_LEN, \n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "\n",
    "reddit_training_steps = (len(reddit_df)//BATCH_SIZE)*NUM_EPOCHS\n",
    "twitter_training_steps = len (twitter_df)//BATCH_SIZE*NUM_EPOCHS\n",
    "\n",
    "model = SuicideClassifier (\n",
    "    output_classes= CLASSES,\n",
    "    training_steps = reddit_training_steps,\n",
    "    warmup_steps=reddit_training_steps/5,\n",
    "    lr=R_LEARNING_RATE, \n",
    "    metrics=['ROC','binary_report']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_trainer_params = generate_trainer_params (\"BERT Implicitly Labeled Reddit v2\", R_LEARNING_RATE)\n",
    "twitter_trainer_params = generate_trainer_params (\"TwitterSI Classification\", T_LEARNING_RATE)\n",
    "\n",
    "reddit_trainer = generate_trainer (reddit_trainer_params)\n",
    "twitter_trainer = generate_trainer (twitter_trainer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_trainer.fit(model, reddit_data_module)\n",
    "reddit_trainer.test()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = SuicideClassifier.load_from_checkpoint(\n",
    "  checkpoint_path = reddit_trainer.checkpoint_callback.best_model_path,\n",
    "  training_steps = twitter_training_steps, \n",
    "  warmup_steps=twitter_training_steps/5,\n",
    "  lr = T_LEARNING_RATE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_trainer.fit (loaded_model, twitter_data_module)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "09054ddbacbdeb414bfc8eb81f54fbfd59bd74d1d62252f07ecfc4bdde97e519"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 ('Profile_SI-ENV': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
