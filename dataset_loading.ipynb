{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "import pytorch_lightning as pl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "text = tokenizer.sep_token.join (sentences)\n",
    "print (text)\n",
    "encoding = tokenizer.encode_plus(text, add_special_tokens = True, truncation = True, padding = \"max_length\", return_attention_mask = True, return_tensors = \"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in encoding:\n",
    "    print (i)\n",
    "    print (type(i))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RedditImplicit (Dataset):\n",
    "    def __init__ (self, reddit_df, tokenizer, max_example_len = 512):\n",
    "        # src is divided into input_ids, token_type_ids, and attention_mask\n",
    "        self.src = tokenizer (\n",
    "            reddit_df['text'].tolist(), \n",
    "            add_special_tokens = True, \n",
    "            truncation = True, \n",
    "            padding = \"max_length\", \n",
    "            return_attention_mask = True, \n",
    "            return_tensors = \"pt\",\n",
    "            max_length = max_example_len\n",
    "        )\n",
    "\n",
    "        self.trg = reddit_df['label'].replace ({'non-suicide':0, 'suicide':1})\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def custom_vocab_preprocessing(df):\n",
    "        return df['text']\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return [\n",
    "            tuple([self.src['input_ids'][idx], self.src['attention_mask'][idx]]),\n",
    "            torch.tensor(self.trg[idx])\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        assert len(self.src['input_ids']) == len(self.trg)\n",
    "        return len(self.trg)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'RedditImplicit ({self.dataset_percent*100}% of full dataset)'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUNNING_DIR = r'C:\\Code\\NLP\\ProfileLevel_SI_Classifier'\n",
    "datasets_dir = os.path.join(RUNNING_DIR, 'Datasets')\n",
    "sns.set_theme()\n",
    "\n",
    "reddit_df = pd.read_csv (os.path.join (datasets_dir, 'Implicitly_Labeled_Suicide_Reddit.csv'))[['text', 'class']]\n",
    "reddit_df = reddit_df.sample (1000).reset_index(drop=True)\n",
    "reddit_df.rename (columns= {'class':'label'}, inplace=True)\n",
    "reddit_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = RedditImplicit (reddit_df, tokenizer, 512)\n",
    "dataloader = DataLoader (dataset, batch_size=32)\n",
    "for (input_ids, attention_mask), trg in dataloader:\n",
    "    print (input_ids.shape, attention_mask.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RedditImplicitDataModule (pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self, data:pd.DataFrame, \n",
    "        tokenizer, splits:list =[1], \n",
    "        max_example_len: int = 512, \n",
    "        shuffle: bool = True,\n",
    "        batch_size:int = 32, \n",
    "        num_workers:int = 0\n",
    "    ):\n",
    "    \n",
    "        super().__init__()\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = batch_size\n",
    "        self.max_example_len = max_example_len\n",
    "        self.shuffle = shuffle\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        self.df_splits = list()\n",
    "        datalen= len(data)\n",
    "        for i, split_percent in enumerate(splits):\n",
    "            prev_split = sum(splits[:i])\n",
    "\n",
    "            self.df_splits.append(\n",
    "                data[\n",
    "                    int(prev_split*datalen):\n",
    "                    int((prev_split *datalen) + \n",
    "                        (split_percent*datalen)\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    def setup (self, stage=None):\n",
    "        self.splits = [\n",
    "            RedditImplicit (\n",
    "                data, \n",
    "                self.tokenizer,\n",
    "                self.max_example_len\n",
    "            )\n",
    "            for data in self.df_splits\n",
    "        ]\n",
    "\n",
    "\n",
    "        if len(self.splits) <= 3:\n",
    "            # complicated syntax making it possible to assign all three at once while padding\n",
    "            # validset/testset if there arent enough splits to fill those values\n",
    "            self.trainset, self.validset, self.testset = [\n",
    "                split for split in self.splits] + [None]*(3 - len(self.splits))\n",
    "\n",
    "            self.datasets = {\n",
    "                'train': self.trainset,\n",
    "                'valid': self.validset,\n",
    "                'test': self.testset\n",
    "            }\n",
    "\n",
    "\n",
    "    def train_dataloader (self):\n",
    "        return DataLoader(\n",
    "            self.trainset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=self.shuffle,\n",
    "            num_workers=self.num_workers\n",
    "        )\n",
    "\n",
    "\n",
    "    def val_dataloader (self):\n",
    "        return DataLoader(\n",
    "            self.validset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=self.shuffle,\n",
    "            num_workers=self.num_workers\n",
    "        )\n",
    "\n",
    "\n",
    "    def test_dataloader (self):\n",
    "        return DataLoader(\n",
    "            self.testset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=self.shuffle,\n",
    "            num_workers=self.num_workers\n",
    "        )\n",
    "\n",
    "data_module = RedditImplicitDataModule (\n",
    "    reddit_df,\n",
    "    tokenizer,\n",
    "    [0.8,0.2],\n",
    "    512, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterLabeledSI (Dataset):\n",
    "    def __init__(self, twitter_df, tokenizer, max_example_len=512):\n",
    "        # src is divided into input_ids, token_type_ids, and attention_mask\n",
    "        twitter_df['tweet'] = twitter_df['tweet'].apply (lambda x: x.strip().lower())\n",
    "        self.src = tokenizer(\n",
    "            twitter_df['tweet'].tolist(),\n",
    "            add_special_tokens=True,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=max_example_len\n",
    "        )\n",
    "\n",
    "        self.trg = twitter_df['label'].tolist()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return [\n",
    "            tuple([self.src['input_ids'][idx], self.src['attention_mask'][idx]]),\n",
    "            torch.tensor(self.trg[idx])\n",
    "        ]\n",
    "\n",
    "    def __len__(self):\n",
    "        assert len(self.src['input_ids']) == len(self.trg)\n",
    "        return len(self.trg)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'LabeledTwitterSI ({self.dataset_percent*100}% of full dataset)'\n",
    "\n",
    "\n",
    "class TwitterDataModule (pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self, data: pd.DataFrame,\n",
    "        tokenizer, splits: list = [1],\n",
    "        max_example_len: int = 512,\n",
    "        shuffle: bool = True,\n",
    "        batch_size: int = 32,\n",
    "        num_workers: int = 0\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = batch_size\n",
    "        self.max_example_len = max_example_len\n",
    "        self.shuffle = shuffle\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "        self.df_splits = list()\n",
    "        datalen = len(data)\n",
    "        for i, split_percent in enumerate(splits):\n",
    "            prev_split = sum(splits[:i])\n",
    "\n",
    "            self.df_splits.append(\n",
    "                data[\n",
    "                    int(prev_split*datalen):\n",
    "                    int((prev_split * datalen) +\n",
    "                        (split_percent*datalen)\n",
    "                        )\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.splits = [\n",
    "            TwitterLabeledSI(\n",
    "                data,\n",
    "                self.tokenizer,\n",
    "                self.max_example_len\n",
    "            )\n",
    "            for data in self.df_splits\n",
    "        ]\n",
    "\n",
    "        if len(self.splits) <= 3:\n",
    "            # complicated syntax making it possible to assign all three at once while padding\n",
    "            # validset/testset if there arent enough splits to fill those values\n",
    "            self.trainset, self.validset, self.testset = [\n",
    "                split for split in self.splits] + [self.splits[-1]]*(3 - len(self.splits))\n",
    "\n",
    "            self.datasets = {\n",
    "                'train': self.trainset,\n",
    "                'valid': self.validset,\n",
    "                'test': self.testset\n",
    "            }\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.trainset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=self.shuffle,\n",
    "            num_workers=self.num_workers\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.validset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.testset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_SI(data_path):\n",
    "    df = pd.read_csv(data_path)\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "tweets_df = _read_SI (os.path.join (datasets_dir, 'Origional Suicidal Tweets.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TwitterLabeledSI (tweets_df , tokenizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Profile_SI-env",
   "language": "python",
   "name": "profile_si-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
